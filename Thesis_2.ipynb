{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877636b5",
   "metadata": {},
   "source": [
    "# Thesis' code\n",
    "\n",
    "## Part 2\n",
    "\n",
    "## 2.1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d2b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b91ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = ['SBER', 'GAZP', 'LKOH', 'NVTK', 'ROSN', 'GMKN', 'TATN', 'SNGS'] \n",
    "\n",
    "base_set = ['rv_d_past', 'rv_w_past', 'rv_m_past']\n",
    "extended_set = base_set + ['high_low_past', 'overnight_past', 'rv_spx', 'rv_bz', 'r_rgbi', 'r_rtsi']\n",
    "\n",
    "features_names = [base_set, extended_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe644eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, startdate = '20140101', enddate = '20240701'):\n",
    "    # this function takes a ticker name (string) as an argument \n",
    "    # returns the table with data on that ticker\n",
    "    path = 'dataset_vol/' + ticker + '.csv'\n",
    "    data = pd.read_csv(path, sep = ';')\n",
    "    data['date']=pd.to_datetime(data['date'])\n",
    "\n",
    "    return data.query('date >= '+startdate+'and date < '+enddate)\n",
    "\n",
    "data = {t: load_data(t) for t in ticker_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1251e",
   "metadata": {},
   "source": [
    "## 2.2 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764ac64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV, ParameterGrid\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6fb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, the functions needed to generate windows are defined\n",
    "def plus_months(y, m, step):\n",
    "    m = m + step\n",
    "    if m > 12:\n",
    "        if m % 12 == 0:\n",
    "            return y + (m // 12), 12\n",
    "        else:\n",
    "            return y + (m // 12), m % 12\n",
    "    else: \n",
    "        return y, m\n",
    "    \n",
    "    \n",
    "def to_sd(y, m):\n",
    "    if m >= 10:\n",
    "        return str(y)+str(m)+'01'\n",
    "    else:\n",
    "        return str(y)+'0'+str(m)+'01'\n",
    "        \n",
    "\n",
    "def get_windows(data_table, features_names, to_scale=False, m_train=24, m_val=3, m_test=3, m_step=3):\n",
    "    # this function given a valid dataset for one of the tickers and the list of features\n",
    "    # creates a list of rolling windows, with each window split into train/validation/test\n",
    "    # for the ease of future use in chosen tuning scheme, a combined dataset (train+validation) is created\n",
    "    # when scaling is required, the scaler is also returned, allowing future descaling and \n",
    "    # enabling comparability of results across scaled and unscaled data\n",
    "    \n",
    "    windows_list = []\n",
    "    \n",
    "    m_window = m_train + m_val + m_test\n",
    "    \n",
    "    start_y = data_table.y.iloc[0]\n",
    "    start_m = data_table.m.iloc[0]\n",
    "    end_y = data_table.y.iloc[-1]\n",
    "    end_m = data_table.m.iloc[-1]\n",
    "\n",
    "    nw = ((12 * (end_y - start_y) + end_m - start_m + 1 - m_window) // m_step) + 1\n",
    "\n",
    "    current_y = start_y\n",
    "    current_m = start_m\n",
    "    for i in range(nw):\n",
    "        windows_list.append({})\n",
    "        \n",
    "        current_start = (current_y, current_m)\n",
    "        \n",
    "        for el in [('train', m_train), ('val', m_val), ('test', m_test)]:\n",
    "            current_end = plus_months(current_start[0], current_start[1], el[1])\n",
    "            query_str = 'date >= '+to_sd(current_start[0], current_start[1])+'and date < '+to_sd(current_end[0], current_end[1])\n",
    "            q = data_table.query(query_str)\n",
    "            \n",
    "            windows_list[-1][el[0]+'_x'] = q[features_names].to_numpy()\n",
    "            windows_list[-1][el[0]+'_y'] = q['rv_d'].to_numpy().reshape(-1, 1)\n",
    "            \n",
    "            current_start = current_end\n",
    "        \n",
    "        windows_list[-1]['comb_x'] = np.concatenate([windows_list[-1]['train_x'], windows_list[-1]['val_x']], axis=0)\n",
    "        windows_list[-1]['comb_y'] = np.concatenate([windows_list[-1]['train_y'], windows_list[-1]['val_y']], axis=0)\n",
    "                                          \n",
    "        windows_list[-1]['test_y_scaler'] = None\n",
    "        current_y, current_m = plus_months(current_y, current_m, m_step)\n",
    "    \n",
    "        if to_scale:\n",
    "            s_train_x = MinMaxScaler()\n",
    "            s_train_y = MinMaxScaler()\n",
    "        \n",
    "            windows_list[-1]['train_x'] = s_train_x.fit_transform(windows_list[-1]['train_x'])\n",
    "            windows_list[-1]['train_y'] = s_train_y.fit_transform(windows_list[-1]['train_y'])\n",
    "        \n",
    "            windows_list[-1]['val_x'] = s_train_x.transform(windows_list[-1]['val_x'])\n",
    "            windows_list[-1]['val_y'] = s_train_y.transform(windows_list[-1]['val_y'])\n",
    "        \n",
    "            s_comb_x = MinMaxScaler()\n",
    "            s_comb_y = MinMaxScaler()\n",
    "        \n",
    "            windows_list[-1]['comb_x'] = s_comb_x.fit_transform(windows_list[-1]['comb_x'])\n",
    "            windows_list[-1]['comb_y'] = s_comb_y.fit_transform(windows_list[-1]['comb_y'])\n",
    "        \n",
    "            windows_list[-1]['test_x'] = s_comb_x.transform(windows_list[-1]['test_x'])\n",
    "        \n",
    "            windows_list[-1]['test_y_scaler'] = s_comb_y\n",
    "        \n",
    "    \n",
    "    return windows_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dcf8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these two simple functions are defined to simplify the future functions\n",
    "def add_metrics(values, preds, errors, metrics):\n",
    "    if 'mse' in metrics:\n",
    "        errors['mse'].append(float(mean_squared_error(values, preds)))\n",
    "        \n",
    "    if 'rmse' in metrics:\n",
    "        errors['rmse'].append(float(root_mean_squared_error(values, preds)))\n",
    "\n",
    "    if 'mae' in metrics:\n",
    "        errors['mae'].append(float(mean_absolute_error(values, preds)))\n",
    "            \n",
    "    if 'mape' in metrics:\n",
    "        errors['mape'].append(float(mean_absolute_percentage_error(values, preds)))\n",
    "\n",
    "    if 'r2' in metrics:\n",
    "        errors['r2'].append(float(r2_score(values, preds)))\n",
    "    \n",
    "    return errors\n",
    "\n",
    "\n",
    "def calculate_metrics(values, preds, metrics):\n",
    "    result = {}\n",
    "    if 'mse' in metrics:\n",
    "        result['mse'] = float(mean_squared_error(values, preds))\n",
    "        \n",
    "    if 'rmse' in metrics:\n",
    "        result['rmse'] = float(root_mean_squared_error(values, preds))\n",
    "\n",
    "    if 'mae' in metrics:\n",
    "        result['mae'] = float(mean_absolute_error(values, preds))\n",
    "            \n",
    "    if 'mape' in metrics:\n",
    "        result['mape'] = float(mean_absolute_percentage_error(values, preds))\n",
    "\n",
    "    if 'r2' in metrics:\n",
    "        result['r2'] = float(r2_score(values, preds))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fcdc6f",
   "metadata": {},
   "source": [
    "### HAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aaff453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f00d8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_har(windows, metrics = ['mse', 'rmse', 'mae', 'mape', 'r2']):\n",
    "    # this function, as others from these category (run_model_name), \n",
    "    # takes as an input the windows, designed to be generated by the previously defined function\n",
    "    # and outputs a tuple constituting a thorough description of the forecast \n",
    "    coefs = []\n",
    "    window_metrics = {m:[] for m in metrics}\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for w in windows:\n",
    "        model = LinearRegression(fit_intercept=True)\n",
    "        model.fit(w['comb_x'], w['comb_y'])\n",
    "        \n",
    "        coefs.append(model.coef_)\n",
    "        \n",
    "        pred = model.predict(w['test_x'])\n",
    "        \n",
    "        if w['test_y_scaler']:\n",
    "            pred = w['test_y_scaler'].inverse_transform(pred)\n",
    "        \n",
    "        preds.append(pred)\n",
    "        targets.append(w['test_y'])\n",
    "        \n",
    "        window_metrics = add_metrics(w['test_y'], pred, window_metrics, metrics)\n",
    "\n",
    "        \n",
    "    coefs = np.array(coefs)\n",
    "    preds = np.concatenate(preds)     \n",
    "    targets = np.concatenate(targets) \n",
    "    result = calculate_metrics(targets, preds, metrics) \n",
    "    \n",
    "    return result, window_metrics, coefs, preds, targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25da9079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:06<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# the piece of code in this cell is used to run the model using the custom function defined above\n",
    "# and save the results (loading and processing the results is given in notebook 3)\n",
    "\n",
    "for t in tqdm(ticker_list):                  \n",
    "    for features, set_name in zip(features_names, ['rv', 'all']):\n",
    "        result, window_metrics, coefs, preds, targets = run_har(get_windows(data[t], features_names=features, to_scale = False), metrics = ['mse', 'rmse', 'mae', 'mape', 'r2'])\n",
    "    \n",
    "        description = {'model': 'HAR',\n",
    "                       'ticker': t,\n",
    "                       'data_set': set_name,\n",
    "                       'rolling_window': '24m3m3m',\n",
    "                       'result': result,\n",
    "                       'window_metrics': window_metrics,\n",
    "                       'coefficients': coefs.tolist(),\n",
    "                       'predictions': preds.tolist()}\n",
    "    \n",
    "        res_path = 'results/HAR/' + t + '_' + set_name + '.json'\n",
    "    \n",
    "        with open(res_path, 'w') as f:\n",
    "            json.dump(description, f)\n",
    "        \n",
    "print('\\nfinished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c9fb3",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c196a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "658bcdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lasso(windows, metrics = ['mse', 'rmse', 'mae', 'mape', 'r2']):\n",
    "    window_metrics = {m:[] for m in metrics}\n",
    "    coefs = []\n",
    "    params = []\n",
    "    parameters = {'alpha': np.logspace(-5, 0, 100)}\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for w in windows:\n",
    "        s_i = [-1]*w['train_x'].shape[0] + [0]*w['val_x'].shape[0]\n",
    "        spl = PredefinedSplit(test_fold = s_i)\n",
    "        \n",
    "        model = Lasso(fit_intercept=True, random_state=0)\n",
    "        gs = GridSearchCV(estimator=model, cv=spl, param_grid=parameters, refit=True, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        gs.fit(w['comb_x'], w['comb_y'])\n",
    "        \n",
    "        params.append(gs.best_params_)\n",
    "        coefs.append(gs.best_estimator_.coef_)\n",
    "\n",
    "        pred = gs.predict(w['test_x'])\n",
    "        if w['test_y_scaler']:\n",
    "            pred = w['test_y_scaler'].inverse_transform(pred.reshape(-1, 1))\n",
    "        preds.append(pred)\n",
    "        targets.append(w['test_y'])\n",
    "        \n",
    "        window_metrics = add_metrics(w['test_y'], pred, window_metrics, metrics)\n",
    "            \n",
    "    coefs = np.array(coefs)\n",
    "    preds = np.concatenate(preds)     \n",
    "    targets = np.concatenate(targets) \n",
    "    result = calculate_metrics(targets, preds, metrics) \n",
    "    \n",
    "    return result, window_metrics, coefs, params, preds, targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99a3e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:43<00:00,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# this code, used to obtain and save the Lasso results, \n",
    "# was run both with to_scale=True and to_scale=False, and a corresponding change in the description and result path\n",
    "# same applies to analogic cells below\n",
    "\n",
    "for t in tqdm(ticker_list):\n",
    "    for features, set_name in zip(features_names, ['rv', 'all']):\n",
    "        result, window_metrics, coefs, params, preds, targets = run_lasso(get_windows(data[t], features_names=features, to_scale = False), metrics = ['mse', 'rmse', 'mae', 'mape', 'r2'])\n",
    "    \n",
    "        description = {'model': 'Lasso',\n",
    "                       'ticker': t,\n",
    "                       'data_scaled': False,\n",
    "                       'data_set': set_name,\n",
    "                       'rolling_window': '24m3m3m',\n",
    "                       'result': result,\n",
    "                       'window_metrics': window_metrics,\n",
    "                       'coefficients': coefs.tolist(),\n",
    "                       'parameters': params,\n",
    "                       'predictions': preds.tolist()}\n",
    "    \n",
    "        res_path = 'results/Lasso/' + t + '_' + set_name + '.json'\n",
    "    \n",
    "        with open(res_path, 'w') as f:\n",
    "            json.dump(description, f)\n",
    "    \n",
    "    \n",
    "print('\\nfinished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0356ad9",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd7d5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "259449a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf(windows, features_names=features_names, metrics = ['mse', 'rmse', 'mae', 'mape', 'r2']):\n",
    "    window_metrics = {m:[] for m in metrics}    \n",
    "    params = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    parameters = {'n_estimators': [100, 200, 300],\n",
    "                  'max_depth': [3, 5]}\n",
    "\n",
    "    for w in tqdm(windows):\n",
    "        s_i = [-1]*w['train_x'].shape[0] + [0]*w['val_x'].shape[0]\n",
    "        spl = PredefinedSplit(test_fold = s_i)\n",
    "                \n",
    "        model = RandomForestRegressor(random_state=0)\n",
    "\n",
    "        gs = GridSearchCV(estimator=model, cv=spl, param_grid=parameters, refit=True, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        gs.fit(w['comb_x'], w['comb_y'].reshape(-1,))\n",
    "        \n",
    "        params.append(gs.best_params_)\n",
    "        \n",
    "        pred = gs.predict(w['test_x'])\n",
    "        if w['test_y_scaler']:\n",
    "            pred = w['test_y_scaler'].inverse_transform(pred.reshape(-1, 1))\n",
    "\n",
    "        preds.append(pred)\n",
    "        targets.append(w['test_y'])\n",
    "        \n",
    "        window_metrics = add_metrics(w['test_y'], pred, window_metrics, metrics)\n",
    "         \n",
    "    preds = np.concatenate(preds)     \n",
    "    targets = np.concatenate(targets) \n",
    "    result = calculate_metrics(targets, preds, metrics) \n",
    "    \n",
    "    return result, window_metrics, params, preds, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39878bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:23<00:00,  1.39it/s]\n",
      "100%|███████████████████████████████████████████| 33/33 [00:39<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBER "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:23<00:00,  1.41it/s]\n",
      "100%|███████████████████████████████████████████| 33/33 [00:35<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAZP "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:24<00:00,  1.36it/s]\n",
      "100%|███████████████████████████████████████████| 33/33 [00:36<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LKOH "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:22<00:00,  1.44it/s]\n",
      "100%|███████████████████████████████████████████| 33/33 [00:34<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVTK "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:21<00:00,  1.53it/s]\n",
      "100%|███████████████████████████████████████████| 33/33 [00:33<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSN "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:21<00:00,  1.54it/s]\n",
      "100%|███████████████████████████████████████████| 33/33 [00:35<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMKN "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:22<00:00,  1.46it/s]\n",
      "100%|███████████████████████████████████████████| 33/33 [00:35<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TATN "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:21<00:00,  1.54it/s]\n",
      "100%|███████████████████████████████████████████| 33/33 [00:33<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNGS \n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for t in ticker_list:      \n",
    "    for features, set_name in zip(features_names, ['rv', 'all']):\n",
    "        result, window_metrics, params, preds, targets = run_rf(get_windows(data[t], features_names=features, to_scale = True), metrics = ['mse', 'rmse', 'mae', 'mape', 'r2'])\n",
    "\n",
    "        description = {'model': 'RF',\n",
    "                       'ticker': t,\n",
    "                       'data_scaled': True,\n",
    "                       'data_set': set_name,\n",
    "                       'rolling_window': '24m3m3m',\n",
    "                       'result': result,\n",
    "                       'window_metrics': window_metrics,\n",
    "                       'parameters': params,\n",
    "                       'predictions': preds.tolist()}\n",
    "    \n",
    "        res_path = 'results/RF_s/' + t + '_' + set_name + '.json'\n",
    "    \n",
    "        with open(res_path, 'w') as f:\n",
    "            json.dump(description, f)\n",
    "    \n",
    "    print(t, end=' ')\n",
    "    \n",
    "print('\\nfinished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ef29f",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f26566d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b2d1a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(windows, features_names=features_names, metrics = ['mse', 'rmse', 'mae', 'mape', 'r2']):\n",
    "    window_metrics = {m:[] for m in metrics}    \n",
    "    params = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    parameters = {'n_estimators': [100, 200, 300],\n",
    "                  'max_depth': [3, 5],\n",
    "                  'learning_rate': np.logspace(-4, -1, 20)}\n",
    "\n",
    "    for w in tqdm(windows):\n",
    "        s_i = [-1]*w['train_x'].shape[0] + [0]*w['val_x'].shape[0]\n",
    "        spl = PredefinedSplit(test_fold = s_i)\n",
    "        \n",
    "        model = xgb.XGBRegressor(random_state=0)                \n",
    "                              \n",
    "        gs = GridSearchCV(estimator=model, cv=spl, param_grid=parameters, refit=True, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        gs.fit(w['comb_x'], w['comb_y'])\n",
    "        \n",
    "        params.append(gs.best_params_)\n",
    "        \n",
    "        pred = gs.predict(w['test_x'])\n",
    "        if w['test_y_scaler']:\n",
    "            pred = w['test_y_scaler'].inverse_transform(pred.reshape(-1, 1))\n",
    "        preds.append(pred)\n",
    "        targets.append(w['test_y'])\n",
    "        \n",
    "        window_metrics = add_metrics(w['test_y'], pred, window_metrics, metrics)\n",
    "\n",
    "    preds = np.concatenate(preds)     \n",
    "    targets = np.concatenate(targets) \n",
    "    result = calculate_metrics(targets, preds, metrics) \n",
    "    \n",
    "    return result, window_metrics, params, preds, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0d72043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:44<00:00,  1.36s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:37<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBER "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:40<00:00,  1.23s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:36<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAZP "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:39<00:00,  1.18s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:35<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LKOH "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:42<00:00,  1.28s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:37<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVTK "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:42<00:00,  1.30s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:37<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSN "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:41<00:00,  1.25s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:38<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMKN "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:42<00:00,  1.28s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:38<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TATN "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:40<00:00,  1.23s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:39<00:00,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNGS \n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for t in ticker_list:      \n",
    "    for features, set_name in zip(features_names, ['rv', 'all']):\n",
    "        result, window_metrics, params, preds, targets = run_xgb(get_windows(data[t], features_names=features, to_scale=True), metrics = ['mse', 'rmse', 'mae', 'mape', 'r2'])\n",
    "        description = {'model': 'XGB',\n",
    "                       'ticker': t,\n",
    "                       'data_scaled': True,\n",
    "                       'data_set': set_name,\n",
    "                       'rolling_window': '24m3m3m',\n",
    "                       'result': result,\n",
    "                       'window_metrics': window_metrics,\n",
    "                       'parameters': params,\n",
    "                       'predictions': preds.tolist()}\n",
    "    \n",
    "        res_path = 'results/XGB_s/' + t + '_' + set_name + '.json'\n",
    "    \n",
    "        with open(res_path, 'w') as f:\n",
    "            json.dump(description, f)\n",
    "    \n",
    "    print(t, end=' ')\n",
    "    \n",
    "print('\\nfinished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f4ba4",
   "metadata": {},
   "source": [
    "### NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "237cdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f541ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden):\n",
    "        super(neural_net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, n_hidden)\n",
    "        self.fc_out = nn.Linear(n_hidden, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = self.fc_out(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "def train(model, device, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) \n",
    "        loss = loss_fn(outputs, targets) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() \n",
    "        \n",
    "                    \n",
    "def test(model, device, test_loader, loss_fn, predict = False, y_scaler=None):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    pred = []\n",
    "    targ = []\n",
    "    with torch.no_grad(): \n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs) \n",
    "            if predict and y_scaler:\n",
    "                outputs = torch.tensor(y_scaler.inverse_transform(outputs)).to(device)\n",
    "            loss = loss_fn(outputs, targets) \n",
    "            test_loss += loss.item() \n",
    "            pred.append(outputs.detach().cpu().numpy())\n",
    "            targ.append(targets.detach().cpu().numpy())\n",
    "    if predict:\n",
    "        return test_loss/len(test_loader), np.concatenate(pred, axis = 0), np.concatenate(targ, axis = 0)\n",
    "\n",
    "    else:\n",
    "        return test_loss/len(test_loader)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c161fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn(windows, n_hidden, metrics = ['mse', 'rmse', 'mae', 'mape', 'r2']):\n",
    "    window_metrics = {m:[] for m in metrics}    \n",
    "    params = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    #errors = []\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "        \n",
    "    n_f = windows[0]['train_x'].shape[1]\n",
    "    batch_size = 32\n",
    "    for w in tqdm(windows):\n",
    "\n",
    "        x_tr = w['train_x'] \n",
    "        y_tr = w['train_y']  \n",
    "        \n",
    "        x_v = w['val_x'] \n",
    "        y_v = w['val_y'] \n",
    "        \n",
    "        x_c = w['comb_x'] \n",
    "        y_c = w['comb_y']  \n",
    "        \n",
    "        x_te = w['test_x'] \n",
    "        y_te = w['test_y']  \n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(x_tr, dtype=torch.float32), \n",
    "                                      torch.tensor(y_tr, dtype=torch.float32))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        val_dataset = TensorDataset(torch.tensor(x_v, dtype=torch.float32), \n",
    "                                    torch.tensor(y_v, dtype=torch.float32))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "                \n",
    "        comb_dataset = TensorDataset(torch.tensor(x_c, dtype=torch.float32), \n",
    "                                    torch.tensor(y_c, dtype=torch.float32))\n",
    "        comb_loader = DataLoader(comb_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        test_dataset = TensorDataset(torch.tensor(x_te, dtype=torch.float32), \n",
    "                                     torch.tensor(y_te, dtype=torch.float32))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)        \n",
    "\n",
    "        best_pars = []\n",
    "        best_e = np.inf\n",
    "        for lr in [1e-3]:\n",
    "            model = neural_net(n_f, n_hidden).to(device)\n",
    "            loss_fn = nn.MSELoss(reduction='mean')\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)  \n",
    "            \n",
    "            for epoch in range(200):\n",
    "                train(model, device, train_loader, loss_fn, optimizer) \n",
    "                if (epoch + 1) % 25 == 0:\n",
    "                    e_v = test(model, device, val_loader, loss_fn) \n",
    "                    if best_e > e_v:\n",
    "                        best_e = e_v\n",
    "                        best_pars = [epoch, lr]\n",
    "\n",
    "        params.append(best_pars)\n",
    "        \n",
    "        model = neural_net(n_f, n_hidden).to(device)\n",
    "        loss_fn = nn.MSELoss(reduction='mean')\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_pars[1])  \n",
    "        \n",
    "        for epoch in range(best_pars[0]+1):\n",
    "            train(model, device, comb_loader, loss_fn, optimizer) \n",
    "            \n",
    "        if w['test_y_scaler']:\n",
    "            err, pred, targ = test(model, device, test_loader, loss_fn, predict=True, y_scaler = w['test_y_scaler'])\n",
    "        else: \n",
    "            err, pred, targ = test(model, device, test_loader, loss_fn, predict=True)\n",
    "        #errors.append(err)\n",
    "        preds.append(pred)\n",
    "        targets.append(targ)\n",
    "        \n",
    "        window_metrics = add_metrics(targ, pred, window_metrics, metrics)\n",
    "\n",
    "    preds = np.concatenate(preds)     \n",
    "    targets = np.concatenate(targets) \n",
    "    result = calculate_metrics(targets, preds, metrics)   \n",
    "    \n",
    "    return result, window_metrics, params, preds, targets #errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b45788ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [02:00<00:00,  3.66s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:55<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [01:54<00:00,  3.47s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:00<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAZP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [01:57<00:00,  3.57s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:00<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LKOH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [01:53<00:00,  3.45s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:56<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVTK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [02:02<00:00,  3.73s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:04<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [01:54<00:00,  3.48s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:54<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMKN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [01:52<00:00,  3.40s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:53<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TATN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [02:01<00:00,  3.67s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [01:56<00:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNGS\n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# here, to_scale and n_hidden could be different, with the corresponding changes to description and path\n",
    "\n",
    "for t in ticker_list:      \n",
    "    for features, set_name in zip(features_names, ['rv', 'all']):\n",
    "        result, window_metrics, params, preds, targets = run_nn(get_windows(data[t], features_names=features, to_scale = True), \n",
    "                                                                n_hidden=16, metrics = ['mse', 'rmse', 'mae', 'mape', 'r2'])\n",
    "        description = {'model': 'NN',\n",
    "                       'ticker': t,\n",
    "                       'data_scaled': True,\n",
    "                       'data_set': set_name,\n",
    "                       'rolling_window': '24m3m3m',\n",
    "                       'n_hidden': 16,\n",
    "                       'max_epoch': 200,\n",
    "                       'result': result,\n",
    "                       'window_metrics': window_metrics,\n",
    "                       'parameters': params,\n",
    "                       'predictions': preds.tolist()}\n",
    "    \n",
    "        res_path = 'results/NN_s_16/' + t + '_' + set_name + '.json'\n",
    "    \n",
    "        with open(res_path, 'w') as f:\n",
    "            json.dump(description, f)\n",
    "    \n",
    "    print(t)\n",
    "    \n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0afad9",
   "metadata": {},
   "source": [
    "### RNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b9dc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b77d7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since rnns require different input format, with the data set split into (overlapping) sequences with fixed legnths,\n",
    "# a separate function for creating windows from the loaded data was defined\n",
    "# it requires same arguments as the ordinary (non rnn) version plus a length of sequence (len_seq) parameter\n",
    "\n",
    "def get_windows_rnn(data_table, features_names, to_scale=False, len_seq=5, m_train = 24, m_val = 3, m_test = 3, m_step = 3):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(data_table)-len_seq+1):\n",
    "        end = i + len_seq\n",
    "        seq_x, seq_y = data_table.iloc[i:end][features_names], data_table.iloc[end-1]['rv_d']\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)    \n",
    "    \n",
    "    windows_list = []\n",
    "    \n",
    "    len_f = len(features_names)\n",
    "    m_window = m_train + m_val + m_test\n",
    "    \n",
    "    start_y = data_table.y.iloc[0]\n",
    "    start_m = data_table.m.iloc[0]\n",
    "    end_y = data_table.y.iloc[-1]\n",
    "    end_m = data_table.m.iloc[-1]\n",
    "\n",
    "    nw = ((12 * (end_y - start_y) + end_m - start_m + 1 - m_window) // m_step) + 1\n",
    "\n",
    "    current_y = start_y\n",
    "    current_m = start_m\n",
    "    \n",
    "    sequence_index = 0\n",
    "    for i in range(nw):\n",
    "        \n",
    "        if sequence_index >= len(x):\n",
    "            break\n",
    "            \n",
    "        windows_list.append({})\n",
    "        \n",
    "        current_start = (current_y, current_m)\n",
    "        \n",
    "        for el in [('train', m_train), ('val', m_val), ('test', m_test)]:\n",
    "            \n",
    "            current_end = plus_months(current_start[0], current_start[1], el[1])\n",
    "            query_str = 'date >= '+to_sd(current_start[0], current_start[1])+'and date < '+to_sd(current_end[0], current_end[1])\n",
    "\n",
    "            query_indices = data_table.query(query_str).index\n",
    "            s_start = max(query_indices[0] - len_seq + 1, 0)\n",
    "            s_end = query_indices[-1] - len_seq + 2\n",
    "            \n",
    "            current_start = current_end\n",
    "            windows_list[-1][el[0]+'_x'] = np.array(x[s_start:s_end])\n",
    "            windows_list[-1][el[0]+'_y'] = np.array(y[s_start:s_end]).reshape(-1, 1)\n",
    "            \n",
    "            \n",
    "        current_y, current_m = plus_months(current_y, current_m, m_step)\n",
    "        \n",
    "        windows_list[-1]['comb_x'] = np.concatenate([windows_list[-1]['train_x'], windows_list[-1]['val_x']], axis=0)\n",
    "        windows_list[-1]['comb_y'] = np.concatenate([windows_list[-1]['train_y'], windows_list[-1]['val_y']], axis=0)\n",
    "                                          \n",
    "        windows_list[-1]['test_y_scaler'] = None\n",
    "\n",
    "        if to_scale:\n",
    "            \n",
    "            s_train_x = MinMaxScaler()\n",
    "            s_train_y = MinMaxScaler()\n",
    "        \n",
    "            windows_list[-1]['train_x'] = s_train_x.fit_transform(np.vstack(windows_list[-1]['train_x'])).reshape(-1, len_seq, len_f)\n",
    "            windows_list[-1]['train_y'] = s_train_y.fit_transform(windows_list[-1]['train_y'].reshape(-1, 1))\n",
    "        \n",
    "            windows_list[-1]['val_x'] = s_train_x.transform(np.vstack(windows_list[-1]['val_x'])).reshape(-1, len_seq, len_f)\n",
    "            windows_list[-1]['val_y'] = s_train_y.transform(np.vstack(windows_list[-1]['val_y']))\n",
    "        \n",
    "            s_comb_x = MinMaxScaler()\n",
    "            s_comb_y = MinMaxScaler()\n",
    "        \n",
    "            windows_list[-1]['comb_x'] = s_comb_x.fit_transform(np.vstack(windows_list[-1]['comb_x'])).reshape(-1, len_seq, len_f)\n",
    "            windows_list[-1]['comb_y'] = s_comb_y.fit_transform(np.vstack(windows_list[-1]['comb_y']))\n",
    "        \n",
    "            windows_list[-1]['test_x'] = s_comb_x.transform(np.vstack(windows_list[-1]['test_x'])).reshape(-1, len_seq, len_f)\n",
    "            windows_list[-1]['test_y'] = np.vstack(windows_list[-1]['test_y'])\n",
    "                \n",
    "            windows_list[-1]['test_y_scaler'] = s_comb_y\n",
    "        \n",
    "    return windows_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2237742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the train and test functions for all the RNNs are the same, they are defined here\n",
    "\n",
    "def train(model, device, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) \n",
    "        loss = loss_fn(outputs, targets) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        running_loss += loss.item() \n",
    "\n",
    "def test(model, device, test_loader, loss_fn, predict = False, y_scaler=None):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    pred = []\n",
    "    targ = []\n",
    "    with torch.no_grad(): \n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs) \n",
    "            if predict and y_scaler:\n",
    "                outputs = torch.tensor(y_scaler.inverse_transform(outputs)).to(device)\n",
    "            loss = loss_fn(outputs, targets) \n",
    "            test_loss += loss.item() \n",
    "            pred.append(outputs.detach().cpu().numpy())\n",
    "            targ.append(targets.detach().cpu().numpy())\n",
    "    if predict:\n",
    "        return test_loss/len(test_loader), np.concatenate(pred, axis = 0), np.concatenate(targ, axis = 0)\n",
    "\n",
    "    else:\n",
    "        return test_loss/len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc8430",
   "metadata": {},
   "source": [
    "### Jordan RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2166ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class jordan_rnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(jordan_rnn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rec = nn.Linear(input_size + 1, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, input_size = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        outputs = []\n",
    "        feedback = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            combined_input = torch.cat((x[:, t, :], feedback), dim=1)  \n",
    "            hidden = torch.relu(self.rec(combined_input))\n",
    "            output = self.fc(hidden)\n",
    "            feedback = output.detach()\n",
    "            outputs.append(output)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1) \n",
    "        return outputs[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e1f710",
   "metadata": {},
   "source": [
    "### Elman NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da09c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class elman_rnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=20, num_layers=1, output_size=1):\n",
    "        super(elman_rnn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        \n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bad36",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75f8f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_rnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=50):\n",
    "        super(lstm_rnn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, 1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6be29a",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c7ff9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gru_rnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=20):\n",
    "        super(gru_rnn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, 1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        \n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8dd7d",
   "metadata": {},
   "source": [
    "### Running RNNs and saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "edb11227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn(rnn, windows, hidden_size=20, metrics = ['mse', 'rmse', 'mae', 'mape', 'r2']):\n",
    "    # this function's first argument is a string with model name, one of 'jnn', 'enn', 'lstm', 'gru'\n",
    "    # other arguments are windows (should be generated by the rnn-specific function) and hidden size\n",
    "    # outputs a tuple describing the forecasts\n",
    "    \n",
    "    window_metrics = {m:[] for m in metrics}    \n",
    "    params = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    #errors = []\n",
    "    \n",
    "    input_size = windows[0]['train_x'].shape[2]\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "        \n",
    "    batch_size = 32\n",
    "\n",
    "    for w in tqdm(windows):\n",
    "        \n",
    "        x_tr = w['train_x'] \n",
    "        y_tr = w['train_y']  \n",
    "        \n",
    "        x_v = w['val_x'] \n",
    "        y_v = w['val_y'] \n",
    "        \n",
    "        x_c = w['comb_x'] \n",
    "        y_c = w['comb_y']  \n",
    "        \n",
    "        x_te = w['test_x'] \n",
    "        y_te = w['test_y']  \n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(x_tr, dtype=torch.float32), \n",
    "                                      torch.tensor(y_tr, dtype=torch.float32))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        val_dataset = TensorDataset(torch.tensor(x_v, dtype=torch.float32), \n",
    "                                    torch.tensor(y_v, dtype=torch.float32))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "                \n",
    "        comb_dataset = TensorDataset(torch.tensor(x_c, dtype=torch.float32), \n",
    "                                    torch.tensor(y_c, dtype=torch.float32))\n",
    "        comb_loader = DataLoader(comb_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        test_dataset = TensorDataset(torch.tensor(x_te, dtype=torch.float32), \n",
    "                                     torch.tensor(y_te, dtype=torch.float32))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)        \n",
    "\n",
    "        best_pars = []\n",
    "        best_e = np.inf\n",
    "        for lr in [1e-3]:\n",
    "            if rnn == 'jnn':\n",
    "                model = jordan_rnn(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "            elif rnn == 'enn':\n",
    "                model = elman_rnn(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "            elif rnn == 'lstm':\n",
    "                model = lstm_rnn(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "            elif rnn == 'gru':\n",
    "                model = gru_rnn(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "                \n",
    "            loss_fn = nn.MSELoss(reduction='mean')\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)  \n",
    "            \n",
    "            for epoch in range(200):\n",
    "                train(model, device, train_loader, loss_fn, optimizer) \n",
    "                if (epoch + 1) % 50 == 0:\n",
    "                    e_v = test(model, device, val_loader, loss_fn) \n",
    "                    if best_e > e_v:\n",
    "                        best_e = e_v\n",
    "                        best_pars = [epoch+1, lr]\n",
    "        \n",
    "        params.append(best_pars)\n",
    "        if rnn == 'jnn':\n",
    "            model = jordan_rnn(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "        elif rnn == 'enn':\n",
    "            model = elman_rnn(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "        elif rnn == 'lstm':\n",
    "            model = lstm_rnn(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "        elif rnn == 'gru':\n",
    "            model = gru_rnn(input_size=input_size, hidden_size=hidden_size).to(device)\n",
    "            \n",
    "        loss_fn = nn.MSELoss(reduction='mean')\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_pars[1])  \n",
    "        \n",
    "        for epoch in range(best_pars[0]):\n",
    "            train(model, device, comb_loader, loss_fn, optimizer) \n",
    "        err, pred, targ = test(model, device, test_loader, loss_fn, predict=True, y_scaler = w['test_y_scaler'])\n",
    "        \n",
    "        #errors.append(err)\n",
    "        preds.append(pred)\n",
    "        targets.append(targ)\n",
    "        \n",
    "        window_metrics = add_metrics(targ, pred, window_metrics, metrics)\n",
    "\n",
    "    preds = np.concatenate(preds)     \n",
    "    targets = np.concatenate(targets) \n",
    "    result = calculate_metrics(targets, preds, metrics)   \n",
    "    \n",
    "    return result, window_metrics, params, preds, targets #errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e2aea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [02:39<00:00,  4.82s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:45<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [02:47<00:00,  5.07s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:48<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAZP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [02:51<00:00,  5.20s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:59<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LKOH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [02:47<00:00,  5.08s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:43<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVTK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [03:10<00:00,  5.78s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:55<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [03:00<00:00,  5.47s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:58<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMKN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [03:01<00:00,  5.50s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [02:56<00:00,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TATN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [03:08<00:00,  5.70s/it]\n",
      "100%|███████████████████████████████████████████| 33/33 [03:06<00:00,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNGS\n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# in this cell, rnn, n_hidden, len_seq, and to_scale were altered (with corresponding changes for description and result path)\n",
    "\n",
    "for t in ticker_list:      \n",
    "    for features, set_name in zip(features_names, ['rv', 'all']):\n",
    "        result, window_metrics, params, preds, targets = run_rnn('jnn', get_windows_rnn(data[t], features, to_scale=True, len_seq=3), hidden_size=20, metrics = ['mse', 'rmse', 'mae', 'mape', 'r2'])\n",
    "        description = {'model': 'Jordan',\n",
    "                       'ticker': t,\n",
    "                       'data_scaled': True,\n",
    "                       'data_set': set_name,\n",
    "                       'rolling_window': '24m3m3m',\n",
    "                       'sequence_length': 3, \n",
    "                       'n_hidden': 20,\n",
    "                       'max_epoch': 200,\n",
    "                       'result': result,\n",
    "                       'window_metrics': window_metrics,\n",
    "                       'parameters': params,\n",
    "                       'predictions': preds.tolist(),\n",
    "                       'targets': targets.tolist()}\n",
    "    \n",
    "        res_path = 'results/J_s_3_20/' + t + '_' + set_name + '.json'\n",
    "    \n",
    "        with open(res_path, 'w') as f:\n",
    "            json.dump(description, f)\n",
    "                \n",
    "    print(t)\n",
    "    \n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f652e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e8c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
